# User Three Points (Verbatim)

Captured verbatim from the user conversation on 2026-02-18.

[1] on stale failure memory
yes, this seems to be a real concern. it's a form of overfitting, in a sense. overfitting to a time in the past, rather than the current state. with that in mind, there needs to be some delineation between the current state of things (i.e., the currrent state of the codebase) and memories. there needs to be some way that the agent knows to always trust its own eyes of what current exists. memories sole purpose are to be a utility. that is a key philisophical point we should put in bold somewhere. the purpose of memories is to be useful to the agent in solving problems it works on. we need some way to for the agent to:
- prioritize what it sees now (the instantaneous truth)
- use memories from the past only to be helpful but understand that they can no longer be true. really, i think we we are getting here is some notion of optionality. not proceding with a claim that every memory provided as context is useful or true or should be taken into consideration but really as a "buffet of optional ideas" or "optional tools in its tool belt" that it can look over and see if anything is useful to pull from, knowing full-well that the truth it sees in the codebase now is what it should be working with. to put in more abstract terms, it's almost as if this memory set has a projection onto the problem at hand (think, dot product similarity for analgogy) and we want it to be able to extract what's useful to it and leave what's not useful to it.


[2] do we need to specify utility for specific problems?
sometimes, there may be memories that are useful for other things... but not for this specific problem... so if we have the reddit style downvote/upvote at the end of each working session where the agent is ask, "please upvote or downvote these memories in the memory set you retrieved from longterm memory" where upvote means it was useful and downvote means it was negatively helpful (e.g., contradiction, led down a wrong path, etc.) -  then it would downvote a memory that maybe be helpful to another problem but not this one. like i'm saying it's possible for one memory to be useful to one problem but not another problem. so we wouldn't want it to be arbitrarily downvoted just because the class of problems it happened to be pulled for didn't find it useful. we are in the realm of utility here, not truth yet. okay, so... maybe this notion of helpful or not helpful can be tied to a specific problelm_id that it was working on. so we'll be able to see "oh, this idea was not helpful for these 3 problems but it was helpful for these 2 problems", as opposed to just a single "total upvotes/downvotes" to distinguish its utility. so this is sort of a record of a memories uitility to other problems... some proxy of measuring transfer learning across tasks.

[3] how to deal with changing facts (truth) and changing utilty values over time. this is directly related to [1] above. there needs to be some efficient way to update memory... otherwise, yes, we will be overfit forever. i do like the idea of immutable episode logs. strangely, i also like the idea of immutable problem descriptions and immutable "this worked" and "this did not work" memories... it gives us a history that may be valuable. but... we just said that we want our memories to update which changing information... okay.. well... that "changing information" is itself something that can be recoreded and stored as a memory. sort of as it's own ontological category. "X changed in the codebase which no makes Y tactic no longer useful" or "Z changed in the codebase which makes H memorized fact no longer true". i think we may have stumbled on something interesting here... a separation between utiltiy and truth. utility updates seem to be related to tactics and procedures... whereas truth updates seem to be related to facts or semantic memory? now, good facts can certainly be useful. in this sense, utility is derived from truth (not exclusively, but sometimes). but, procedures or tactics? those aren't really statements of truth. they are algorithms: processes, which have utilitiy toward a goal (to solve a problem).
